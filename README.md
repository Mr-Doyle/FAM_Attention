# FAM_Attention
We propose a novel attention mechanism called Factor Analysis-based Multi-head (FAM) Attention, which combines the theory of explorative factor analysis and word embedding.
![FAM_Architecture](https://github.com/user-attachments/assets/2edbdb7a-e391-4924-92ad-d06e44f8d539)
